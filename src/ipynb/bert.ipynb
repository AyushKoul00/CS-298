{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "BERT_MODEL_TYPE = 'distilbert'\n",
    "MALWARE_DIR = Path('malware_samples/')  # Directory containing malware type folders\n",
    "SAVED_MODELS_DIR = Path(f'saved_models/{BERT_MODEL_TYPE}/')\n",
    "os.makedirs(SAVED_MODELS_DIR, exist_ok=True)\n",
    "MALWARE_TYPES = ['winwebsec', 'zbot', 'zeroaccess']  # Malware type folder names\n",
    "MAX_SAMPLES_PER_TYPE = [2] * len(MALWARE_TYPES) # Set to -1 to read all files, or set to maximum number of files per folder\n",
    "MAX_CHUNK_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(opcodes, tokenizer, max_length=512, overlap_percent=0.1):\n",
    "    \"\"\"\n",
    "    Tokenize all opcodes into subwords first, then split into chunks with overlap\n",
    "    \n",
    "    Args:\n",
    "        opcodes (list): List of opcode strings\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        max_length (int): Maximum sequence length\n",
    "        overlap_percent (float): Overlap percentage between chunks\n",
    "    \n",
    "    Returns:\n",
    "        BatchEncoding: Contains input_ids, attention_mask, etc.\n",
    "    \"\"\"\n",
    "    # Tokenize all opcodes into subwords using list comprehension\n",
    "    all_tokens = [token for opcode in opcodes for token in tokenizer.tokenize(opcode)]\n",
    "\n",
    "    # Calculate chunking parameters\n",
    "    chunk_size = max_length - 2  # Account for [CLS] and [SEP]\n",
    "    step = max(1, int(chunk_size * (1 - overlap_percent)))\n",
    "    \n",
    "    # Generate overlapping chunks using walrus operator\n",
    "    token_chunks = []\n",
    "    start_idx = 0\n",
    "    while (current_chunk := all_tokens[start_idx:start_idx + chunk_size]):\n",
    "        token_chunks.append(current_chunk)\n",
    "        start_idx += step\n",
    "\n",
    "    # Convert token chunks to model inputs\n",
    "    return tokenizer(\n",
    "        token_chunks,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "def generate_malware_embeddings(model_name='bert-base-uncased', overlap_percent=0.1):\n",
    "    \"\"\"\n",
    "    Generate embeddings using BERT with overlapping token chunks\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).eval()\n",
    "    embeddings = {}\n",
    "\n",
    "    # Process each malware type\n",
    "    for malware_type, max_samples in zip(MALWARE_TYPES, MAX_SAMPLES_PER_TYPE):\n",
    "        curr_dir = MALWARE_DIR / malware_type\n",
    "        if not curr_dir.is_dir():\n",
    "            continue  # Skip if the directory doesn't exist\n",
    "\n",
    "        # Convert to a list so we can reuse it\n",
    "        filepaths = list(curr_dir.glob('*.txt'))\n",
    "\n",
    "        # Optionally limit the number of samples\n",
    "        if max_samples > 0 and max_samples < len(filepaths):\n",
    "            filepaths = filepaths[:max_samples]\n",
    "\n",
    "        for filepath in filepaths:\n",
    "            # Read opcodes with walrus operator\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                opcodes = [l for line in f if (l := line.strip())]\n",
    "\n",
    "            # Tokenize and chunk with overlap\n",
    "            encoded_chunks = tokenize_and_chunk(\n",
    "                opcodes=opcodes,\n",
    "                tokenizer=tokenizer,\n",
    "                max_length=MAX_CHUNK_LENGTH,\n",
    "                overlap_percent=overlap_percent\n",
    "            )\n",
    "\n",
    "            # Process all chunks in batch with inference mode\n",
    "            with torch.inference_mode():\n",
    "                outputs = model(**encoded_chunks)\n",
    "\n",
    "            # Calculate valid token mask\n",
    "            input_ids = encoded_chunks['input_ids']\n",
    "            print(input_ids.shape)\n",
    "\n",
    "            valid_mask = (\n",
    "                (input_ids != tokenizer.cls_token_id) &\n",
    "                (input_ids != tokenizer.sep_token_id) &\n",
    "                (input_ids != tokenizer.pad_token_id)\n",
    "            )\n",
    "\n",
    "            # # Mean Chunk Embeddings\n",
    "            # chunk_embeddings = [\n",
    "            #     outputs.last_hidden_state[i][mask].mean(dim=0).numpy()\n",
    "            #     for i, mask in enumerate(valid_mask)\n",
    "            #     if mask.any()\n",
    "            # ]\n",
    "\n",
    "            # CLS Chunk Embeddings\n",
    "            chunk_embeddings = [\n",
    "                outputs.last_hidden_state[i][0].cpu().numpy()  # CLS token\n",
    "                for i in range(input_ids.shape[0])\n",
    "                if valid_mask[i].any()  # Still filter empty chunks\n",
    "            ]\n",
    "\n",
    "            # Average across chunks (no normalization)\n",
    "            file_embedding = np.mean(chunk_embeddings, axis=0) if chunk_embeddings \\\n",
    "                else np.zeros(model.config.hidden_size)\n",
    "            \n",
    "            embeddings[(malware_type, filepath.name)] = file_embedding\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([6, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([31, 512])\n",
      "torch.Size([30, 512])\n",
      "Generated embeddings for 6 files\n",
      "('winwebsec', '0009d99691e8eed99c7dd1500e07cda336d54260.asm.txt'): (768,)\n",
      "('winwebsec', '00113d9802cca3deba19cf9daa17f1c2269de2b8.asm.txt'): (768,)\n",
      "('zbot', '002c3a4a12eb9cdc80754e4cddccbc98e5769392.asm.txt'): (768,)\n",
      "('zbot', '003c10125d80ba6cdbb05bc9aa047c7dbaa6b7ff.asm.txt'): (768,)\n",
      "('zeroaccess', '00b17ae7e516d7b317434a3a72cce6872faa83e3.asm.txt'): (768,)\n",
      "('zeroaccess', '00eff34096389925a203690299efad031c106826.asm.txt'): (768,)\n"
     ]
    }
   ],
   "source": [
    "embeddings = generate_malware_embeddings(\n",
    "    model_name=f'{BERT_MODEL_TYPE}-base-uncased',\n",
    "    overlap_percent=0.2  # 20% overlap between token chunks\n",
    ")\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings)} files\")\n",
    "\n",
    "for key, E in embeddings.items():\n",
    "    # print(f'{filename}:', E, sep=\"\\n\")\n",
    "    print(f'{key}: {E.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (SAVED_MODELS_DIR / 'mean_embedding_per_file.pkl').open('wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs298",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
